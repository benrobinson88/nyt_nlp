{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob import classifiers\n",
    "import pprint\n",
    "from numpy import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to conduct our analysis on a range of news sources, across mediums and political positions. Roughly, we've organized them into three classes:  Newspapers (e.g. USA Today), Networks (e.g. CNN), and primarily online sources (e.g. Reuters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = ['http://www.usatoday.com','http://www.wsj.com','http://www.nytimes.com',\\\n",
    "              'http://www.washingtonpost.com', 'http://www.latimes.com', 'http://www.nypost.com',\\\n",
    "             'http://www.newsday.com', 'http://www.chicagotribune.com', 'http://www.nydailynews.com',\\\n",
    "              'http://www.denverpost.com', 'http://www.chron.com', 'http://www.dallasnews.com',\\\n",
    "              'http://www.bostonglobe.com', 'http://www.seattletimes.com', 'http://www.tampabay.com']\n",
    "\n",
    "networks = ['http://www.cnn.com', 'http://www.msnbc.com', 'http://www.foxnews.com', 'http://abcnews.go.com',\\\n",
    "           'http://www.cbsnews.com']\n",
    "\n",
    "online_primary = ['http://www.reuters.com','http://news.yahoo.com', 'http://www.news.aol.com','http://www.huffingtonpost.com',\\\n",
    "              'http://www.theatlantic.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to search for immigration stories, we're going to use a list of immigration-related root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_roots = ['immig', 'sessions', 'border', 'migran', 'nielsen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's analyze the newspapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for http://www.usatoday.com\n",
      "{'nouns': WordList([u'jeff sessions', u'don \\u2019 t', u'separate parents']),\n",
      " 'polarity': 0.0,\n",
      " 'subjectivity': 0.0}\n",
      "{'nouns': WordList([u'facebook', u'campaign raises $ 5m', u'reunite immigrant families']),\n",
      " 'polarity': 0.0,\n",
      " 'subjectivity': 0.0}\n",
      "{}\n",
      "Summary for http://www.wsj.com\n",
      "{'nouns': WordList([u'trump', u'broad bill legislation', u'migrant children']),\n",
      " 'polarity': -0.012500000000000002,\n",
      " 'subjectivity': 0.3375000000000001}\n",
      "{}\n",
      "Summary for http://www.nytimes.com\n",
      "{'nouns': WordList([u'trump', u'\\u2019 s policy', u'immigrant children', u'mr. trump']),\n",
      " 'polarity': 0.033333333333333326,\n",
      " 'subjectivity': 0.5}\n",
      "{'nouns': WordList([u'trump', u'border policy']),\n",
      " 'polarity': 0.5,\n",
      " 'subjectivity': 0.5}\n",
      "{}\n",
      "Summary for http://www.washingtonpost.com\n",
      "{}\n",
      "Summary for http://www.latimes.com\n",
      "{}\n",
      "Summary for http://www.nypost.com\n",
      "{}\n",
      "Summary for http://www.newsday.com\n",
      "{'nouns': WordList([u'spin cycle cuomo', u'ny', u'immigrant policy']),\n",
      " 'polarity': 0.0,\n",
      " 'subjectivity': 0.0}\n",
      "{}\n",
      "Summary for http://www.chicagotribune.com\n",
      "{'nouns': WordList([u'starbucks', u'u.s.', u'racial-bias training sessions', u'mandatory session', u'high-profile arrest', u'black men', u'philadelphia starbucks', u'business partner', u'white employee']),\n",
      " 'polarity': -0.07916666666666666,\n",
      " 'subjectivity': 0.13333333333333333}\n",
      "{'nouns': WordList([u'lady \\u2014', u'melania', u'immigrant families', u'u.s.-mexico', u'rare moment', u'bipartisan unity', u'small sorority', u'presidential spouses', u'laura bush']),\n",
      " 'polarity': -0.10833333333333334,\n",
      " 'subjectivity': 0.4138888888888889}\n",
      "{'nouns': WordList([u'old warehouse', u'texas', u'immigrant children', u'scattered', u'large foil sheets']),\n",
      " 'polarity': 0.15714285714285714,\n",
      " 'subjectivity': 0.3142857142857143}\n",
      "{}\n",
      "Summary for http://www.nydailynews.com\n",
      "{}\n",
      "Summary for http://www.denverpost.com\n",
      "{}\n",
      "Summary for http://www.chron.com\n",
      "{}\n",
      "Summary for http://www.dallasnews.com\n",
      "{}\n",
      "Summary for http://www.bostonglobe.com\n",
      "{'nouns': WordList([u'thomas hodgson', u'jail staff', u'salvadoran', u'immigrant \\u2019 s', u'constitutional rights', u'civil detainer']),\n",
      " 'polarity': -0.1,\n",
      " 'subjectivity': 0.0}\n",
      "{'nouns': WordList([u'country supports', u'so-called zero-tolerance policy', u'dramatic increase', u'southern border']),\n",
      " 'polarity': -0.14444444444444443,\n",
      " 'subjectivity': 0.3666666666666667}\n",
      "{'nouns': WordList([u'racial subjugation', u'white supremacy', u'\\u2019 re', u'border isn \\u2019 t', u'\\u2019 s']),\n",
      " 'polarity': 0.0,\n",
      " 'subjectivity': 0.0}\n",
      "{}\n",
      "Summary for http://www.seattletimes.com\n",
      "{}\n",
      "Summary for http://www.tampabay.com\n",
      "{}\n",
      "Overall Summary\n",
      "{'http://www.bostonglobe.com': {},\n",
      " 'http://www.chicagotribune.com': {},\n",
      " 'http://www.chron.com': {},\n",
      " 'http://www.dallasnews.com': {},\n",
      " 'http://www.denverpost.com': {},\n",
      " 'http://www.latimes.com': {},\n",
      " 'http://www.newsday.com': {},\n",
      " 'http://www.nydailynews.com': {},\n",
      " 'http://www.nypost.com': {},\n",
      " 'http://www.nytimes.com': {},\n",
      " 'http://www.seattletimes.com': {},\n",
      " 'http://www.tampabay.com': {},\n",
      " 'http://www.usatoday.com': {},\n",
      " 'http://www.washingtonpost.com': {},\n",
      " 'http://www.wsj.com': {}}\n"
     ]
    }
   ],
   "source": [
    "newspaper_summary = {}\n",
    "for paper in newspapers:\n",
    "    summary ={}\n",
    "    print \"Summary for %s\" %paper\n",
    "    r = requests.get(paper)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    stories_to_read = []\n",
    "    for item in paragraphs:\n",
    "        stories_to_read.append(item.get_text())\n",
    "    for story in stories_to_read:\n",
    "        story_summary = {}\n",
    "        text_story = TextBlob(story)\n",
    "        for phrase in immig_roots:\n",
    "            if str(text_story.noun_phrases).find(phrase) > 0:\n",
    "                story_summary['nouns'] = text_story.noun_phrases\n",
    "                story_summary['polarity'] = text_story.sentiment.polarity\n",
    "                story_summary['subjectivity'] = text_story.sentiment.subjectivity\n",
    "        if len(story_summary)>0:\n",
    "            pprint.pprint(story_summary)\n",
    "    pprint.pprint(summary)\n",
    "    newspaper_summary[paper] = summary\n",
    "print \"Overall Summary\"\n",
    "pprint.pprint(newspaper_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on http://www.usatoday.com\n",
      "Subjectivity Score for http://www.usatoday.com is 0.0\n",
      "Polarity Score for http://www.usatoday.com is 0.0\n",
      "Working on http://www.wsj.com\n",
      "Subjectivity Score for http://www.wsj.com is 0.1125\n",
      "Polarity Score for http://www.wsj.com is -0.00416666666667\n",
      "Working on http://www.nytimes.com\n",
      "Subjectivity Score for http://www.nytimes.com is 0.207792207792\n",
      "Polarity Score for http://www.nytimes.com is 0.0194805194805\n",
      "Working on http://www.washingtonpost.com\n",
      "Subjectivity Score for http://www.washingtonpost.com is Not enough data\n",
      "Polarity Score for http://www.washingtonpost.com is Not enough data\n",
      "Working on http://www.latimes.com\n",
      "Subjectivity Score for http://www.latimes.com is Not enough data\n",
      "Polarity Score for http://www.latimes.com is Not enough data\n",
      "Working on http://www.nypost.com\n",
      "Subjectivity Score for http://www.nypost.com is Not enough data\n",
      "Polarity Score for http://www.nypost.com is Not enough data\n",
      "Working on http://www.newsday.com\n",
      "Subjectivity Score for http://www.newsday.com is 0.0\n",
      "Polarity Score for http://www.newsday.com is 0.0\n",
      "Working on http://www.chicagotribune.com\n",
      "Subjectivity Score for http://www.chicagotribune.com is 0.35641025641\n",
      "Polarity Score for http://www.chicagotribune.com is 0.0736698717949\n",
      "Working on http://www.nydailynews.com\n",
      "Subjectivity Score for http://www.nydailynews.com is 0.0\n",
      "Polarity Score for http://www.nydailynews.com is 0.0\n",
      "Working on http://www.denverpost.com\n",
      "Subjectivity Score for http://www.denverpost.com is Not enough data\n",
      "Polarity Score for http://www.denverpost.com is Not enough data\n",
      "Working on http://www.chron.com\n",
      "Subjectivity Score for http://www.chron.com is Not enough data\n",
      "Polarity Score for http://www.chron.com is Not enough data\n",
      "Working on http://www.dallasnews.com\n",
      "Subjectivity Score for http://www.dallasnews.com is Not enough data\n",
      "Polarity Score for http://www.dallasnews.com is Not enough data\n",
      "Working on http://www.bostonglobe.com\n",
      "Subjectivity Score for http://www.bostonglobe.com is 0.0733333333333\n",
      "Polarity Score for http://www.bostonglobe.com is -0.0688888888889\n",
      "Working on http://www.seattletimes.com\n",
      "Subjectivity Score for http://www.seattletimes.com is Not enough data\n",
      "Polarity Score for http://www.seattletimes.com is Not enough data\n",
      "Working on http://www.tampabay.com\n",
      "Subjectivity Score for http://www.tampabay.com is Not enough data\n",
      "Polarity Score for http://www.tampabay.com is Not enough data\n"
     ]
    }
   ],
   "source": [
    "newspaper_summary = {}\n",
    "overall_polarity = []\n",
    "for paper in newspapers:\n",
    "    subjectivity = 0\n",
    "    polarity = 0\n",
    "    num_stories = 0\n",
    "    print \"Working on %s\" %paper\n",
    "    r = requests.get(paper)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    stories_to_read = []\n",
    "    for item in paragraphs:\n",
    "        stories_to_read.append(item.get_text())\n",
    "    for story in stories_to_read:\n",
    "        text_story = TextBlob(story)\n",
    "        words_found = 0\n",
    "        words_checked = 0\n",
    "        while words_found < 1 and words_checked < len(immigration_roots):\n",
    "            for phrase in immig_roots:\n",
    "                if str(text_story.noun_phrases).find(phrase) > 0:\n",
    "                    polarity += text_story.sentiment.polarity\n",
    "                    subjectivity += text_story.sentiment.subjectivity\n",
    "                    num_stories +=1\n",
    "                    words_found +=1\n",
    "                words_checked +=1\n",
    "    if num_stories > 0:\n",
    "        paper_subjectivity = subjectivity/num_stories\n",
    "        paper_polarity = polarity/num_stories\n",
    "        overall_polarity.append(paper_polarity)\n",
    "    else:\n",
    "        paper_subjectivity = \"Not enough data\"\n",
    "        paper_polarity = \"Not enough data\"\n",
    "    print \"Subjectivity Score for %s is %s\" %(paper, paper_subjectivity)\n",
    "    print \"Polarity Score for %s is %s\" %(paper, paper_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall polarity score for this topic is 0.002870690817119389\n"
     ]
    }
   ],
   "source": [
    "grand_polarity = mean(overall_polarity)\n",
    "print \"Overall polarity score for this topic is %s\" %grand_polarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
